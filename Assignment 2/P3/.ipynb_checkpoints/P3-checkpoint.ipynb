{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3\n",
    "## 1D CNN forward and backward propagation with numpy to replicate PyTorch\n",
    "If $y$ is the output after filtering $x$ with $w$ and\n",
    "\n",
    "$N$ = Number of samples <br>\n",
    "$C_{in}$ = Number of input channels = Kernel depth<br>\n",
    "$C_{out}$ = Number of output channels = Number of filters<br>\n",
    "$l_{in}$ = Input width <br>\n",
    "$l_{out}$ = $\\frac{l_{in} - K}{s} + 1$  = Output width<br>\n",
    "$K$ = Kernel length <br>\n",
    "$s$ = Stride <br>\n",
    "\n",
    "$x.shape = (N,C_{in}, l_{in})$ <br>\n",
    "$w.shape = (C_{out},C_{in},K)$ <br>\n",
    "$y.shape = (N,C_{out}, l_{out})$ <br>\n",
    "\n",
    "then $y(n,c,i)$ is the $i$-th output of $c$-th output channel for $n$-th sample\n",
    "$$ y(n,c,i) = \\Big[ \\sum_{k = 1}^{K} \\sum_{j=1}^{C_{in}} w(c,j,k) x(n,j,si + k)\\Big] + b(c) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\sum_{n = 1}^{N} \\sum_{i = 1}^{l_{out}} \\frac{\\partial L}{\\partial b(c)} = \\sum_{n = 1}^{N} \\sum_{i = 1}^{l_{out}} \\frac{\\partial L}{\\partial y(n,c,i)} \\frac{\\partial  y(n,c,i)}{\\partial b(c)} = \\sum_{n = 1}^{N} \\sum_{i = 1}^{l_{out}} \\frac{\\partial L}{\\partial y(n,c,i)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\sum_{n = 1}^{N} \\sum_{i = 1}^{l_{out}} \\frac{\\partial L}{\\partial w(c^\\prime,j^\\prime, k^\\prime)} =  \\sum_{n = 1}^{N} \\sum_{i = 1}^{l_{out}} \\frac{\\partial  L} {\\partial  y(n,c^\\prime,i)} \\frac{\\partial  y(n,c^\\prime,i)}{\\partial w(c^\\prime,j^\\prime, k^\\prime)} = \\sum_{n = 1}^{N} \\sum_{i = 1}^{l_{out}} \\frac{\\partial  L} {\\partial  y(n,c^\\prime,i)}  x(n,j^\\prime,si + k^\\prime) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$  \\sum_{c = 1}^{C_{in}} \\sum_{i = 1}^{l_{out}} \\frac{\\partial L}{\\partial x(n^\\prime,j^\\prime, si+k^\\prime)} =\\sum_{c = 1}^{C_{in}} \\sum_{i = 1}^{l_{out}} \\frac{\\partial L}{\\partial y(n^\\prime,c, i)} \\frac{\\partial y(n^\\prime,c, i)}{x(n^\\prime,j^\\prime, si+k^\\prime)} = \\sum_{c = 1}^{C_{in}} \\sum_{i = 1}^{l_{out}} \\frac{\\partial L}{\\partial y(n^\\prime,c, i)} w(c,j^{\\prime} k^{\\prime})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% layers.py\n",
    "#%%\n",
    "import numpy as np\n",
    "#%%\n",
    "class Conv1D:\n",
    "    \n",
    "    def __init__(self, in_channel, out_channel, kernal_size, stride):\n",
    "        self.in_channel = in_channel\n",
    "        self.out_channel = out_channel\n",
    "        self.kernal_size  = kernal_size\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.k = 1/(self.in_channel*self.kernal_size)\n",
    "        self.W = np.random.uniform(-np.sqrt(self.k), np.sqrt(self.k), (self.out_channel, self.in_channel, self.kernal_size))\n",
    "        self.b = np.random.uniform(-np.sqrt(self.k), np.sqrt(self.k), self.out_channel)\n",
    "           \n",
    "\n",
    "    def __call__(self, data):\n",
    "        self.data = data\n",
    "        self.samples, _, self.in_width = np.shape(self.data)           \n",
    "        self.out_width = ((self.in_width-self.kernal_size)//self.stride)+1\n",
    "        self.fprop = np.zeros((self.samples, self.out_channel, self.out_width))       \n",
    "        \n",
    "        for s in range(self.samples):\n",
    "            for c in range(self.out_channel):\n",
    "                for i in range(self.out_width):\n",
    "                    self.fprop[s,c,i] = np.sum(self.data[s,:,(i*self.stride):((i*self.stride)+self.kernal_size)]*self.W[c]) + self.b[c]\n",
    "        return self.fprop\n",
    "\n",
    "   \n",
    "    def backward(self, delta):        \n",
    "        self.dW = np.zeros(self.W.shape)\n",
    "        self.db = np.zeros(self.b.shape)\n",
    "        self.dX = np.zeros(self.data.shape)\n",
    "        \n",
    "        \n",
    "        for c in range(self.out_channel):\n",
    "            for n in range(self.samples):\n",
    "                for i in range(self.out_width):\n",
    "                    self.db[c] += delta[n,c,i]\n",
    "        \n",
    "        for c in range(self.out_channel):\n",
    "                for j in range(self.in_channel):\n",
    "                    for k in range(self.kernal_size):\n",
    "                        for n in range(self.samples):\n",
    "                            for i in range(self.out_width):\n",
    "                                self.dW[c,j,k] += delta[n,c,i]*self.data[n,j,(self.stride*i)+k]\n",
    "                            \n",
    "        for n in range(self.samples):\n",
    "            for j in range(self.in_channel):\n",
    "                for k in range(self.kernal_size):\n",
    "                    for c in range(self.out_channel):\n",
    "                        for i in range(self.out_width):\n",
    "                            self.dX[n,j,((self.stride*i)+k)] += delta[n,c,i]*self.W[c,j,k]\n",
    "                                  \n",
    "        return self.db, self.dW, self.dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.220446049250313e-16\n",
      "3.3306690738754696e-16\n",
      "5.329070518200751e-15\n",
      "1.7763568394002505e-15\n"
     ]
    }
   ],
   "source": [
    "#%% main_file.py\n",
    "import numpy as np\n",
    "# Torch Library\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable \n",
    "# My Library\n",
    "import layers as my\n",
    "#%% Create Layers\n",
    "np.random.seed(10)\n",
    "net1 = my.Conv1D(8,12,3,2)          # mylib\n",
    "net2 = torch.nn.Conv1d(8,12,3,2)    # torch\n",
    "#%% Initialize Layers\n",
    "x1 = np.random.rand(3,8,20)                             # mylib\n",
    "x2 = Variable(torch.tensor(x1), requires_grad = True)   # torch\n",
    "net2.weight = nn.Parameter(torch.tensor(net1.W))        # torch\n",
    "net2.bias = nn.Parameter(torch.tensor(net1.b))          # torch\n",
    "#%% Forward Propagation \n",
    "y_mylib = net1(x1)                                      # mylib\n",
    "y_torch = net2(x2)                                      # torch\n",
    "y_torch_np = y_torch.detach().numpy()                   # torch\n",
    "#%%\n",
    "b , c, w = y_mylib.shape\n",
    "delta = np.random.randn(b,c,w) \n",
    "db_mylib, dW_mylib, dX_mylib = net1.backward(delta)\n",
    "#%%\n",
    "y_torch.backward(torch.tensor(delta))\n",
    "dW_torch = net2.weight.grad.detach().numpy()\n",
    "db_torch = net2.bias.grad.detach().numpy()\n",
    "dX_torch = x2.grad.detach().numpy()\n",
    "#%% Compare\n",
    "def compare(x,y):\n",
    "    return print(abs(x-y).max())\n",
    "#%%\n",
    "compare(y_mylib, y_torch_np)\n",
    "compare(dX_mylib, dX_torch)\n",
    "compare(dW_mylib, dW_torch)\n",
    "compare(db_mylib, db_torch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
